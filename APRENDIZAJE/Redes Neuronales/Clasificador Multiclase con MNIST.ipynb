{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDmhJpGIM1xvV4uKqt4Rq0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"s5P_bTyyKH_j"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"700d581b"},"source":["# Task\n","Investiga el dataset MNIST, crea código con PyTorch y W&B para entrenar un clasificador multiclase con MNIST. Entrena un modelo baseline sin regularización ni normalización, luego agrega normalización y regularizaciones (L2, dropout) y compara los resultados."]},{"cell_type":"markdown","metadata":{"id":"2c0ba6ff"},"source":["## Investigación del dataset mnist\n","\n","### Subtask:\n","Realizar una investigación sobre el dataset MNIST, su estructura, contenido y uso común en tareas de clasificación de imágenes.\n"]},{"cell_type":"markdown","metadata":{"id":"2942af34"},"source":["**Reasoning**:\n","The subtask requires researching the MNIST dataset. I will use a code block to document the key findings from the research about the dataset's structure, content, common usage, and characteristics.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9212d6e0","executionInfo":{"status":"ok","timestamp":1757114206800,"user_tz":360,"elapsed":16,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"cc7aec8e-9e5f-496a-eda6-9de3771c5597"},"source":["# Research on MNIST dataset\n","print(\"MNIST Dataset Research:\")\n","print(\"- Structure: Contains grayscale images of handwritten digits (0-9).\")\n","print(\"- Image size: 28x28 pixels.\")\n","print(\"- Number of classes: 10 (one for each digit).\")\n","print(\"- Dataset size: 60,000 training examples, 10,000 test examples.\")\n","print(\"- Content: Images of handwritten digits and corresponding labels.\")\n","print(\"- Common usage: Benchmark for image classification tasks, especially for evaluating new models and techniques.\")\n","print(\"- Common models: Convolutional Neural Networks (CNNs), Support Vector Machines (SVMs), traditional machine learning models.\")\n","print(\"- Characteristics: Relatively simple dataset, low resolution images, variations in handwriting style.\")"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["MNIST Dataset Research:\n","- Structure: Contains grayscale images of handwritten digits (0-9).\n","- Image size: 28x28 pixels.\n","- Number of classes: 10 (one for each digit).\n","- Dataset size: 60,000 training examples, 10,000 test examples.\n","- Content: Images of handwritten digits and corresponding labels.\n","- Common usage: Benchmark for image classification tasks, especially for evaluating new models and techniques.\n","- Common models: Convolutional Neural Networks (CNNs), Support Vector Machines (SVMs), traditional machine learning models.\n","- Characteristics: Relatively simple dataset, low resolution images, variations in handwriting style.\n"]}]},{"cell_type":"markdown","metadata":{"id":"5f4a2b67"},"source":["## Configuración del entorno\n","\n","### Subtask:\n","Instalar las librerías necesarias (PyTorch, Weights & Biases).\n"]},{"cell_type":"markdown","metadata":{"id":"b842648e"},"source":["**Reasoning**:\n","Install the required libraries using pip.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b87f5f42","executionInfo":{"status":"ok","timestamp":1757114226618,"user_tz":360,"elapsed":10659,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"c8d75132-4f40-497d-f57a-798ecb614fda"},"source":["!pip install torch torchvision wandb"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.2.1)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.7)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.35.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"9da2a6a7"},"source":["## Carga y preprocesamiento de datos\n","\n","### Subtask:\n","Cargar el dataset MNIST usando PyTorch y preparar los dataloaders para entrenamiento y prueba.\n"]},{"cell_type":"markdown","metadata":{"id":"d79c4d06"},"source":["**Reasoning**:\n","Import necessary libraries and define the transformation to load and prepare the MNIST dataset.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c979d04a","executionInfo":{"status":"ok","timestamp":1757114253118,"user_tz":360,"elapsed":16692,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"19c972cf-6dfd-4f36-b633-42514c2ac900"},"source":["import torch\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","# Define a transformation to convert images to tensors\n","transform = transforms.ToTensor()\n","\n","# Load the MNIST training dataset\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","\n","# Load the MNIST test dataset\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# Create a DataLoader for the training dataset\n","batch_size = 64  # You can adjust the batch size\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Create a DataLoader for the test dataset\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","print(\"MNIST datasets loaded and DataLoaders created.\")\n","print(f\"Number of training batches: {len(train_loader)}\")\n","print(f\"Number of test batches: {len(test_loader)}\")"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 479kB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 4.42MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 5.36MB/s]"]},{"output_type":"stream","name":"stdout","text":["MNIST datasets loaded and DataLoaders created.\n","Number of training batches: 938\n","Number of test batches: 157\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"68059f4d"},"source":["## Definición del modelo baseline\n","\n","### Subtask:\n","Definir una red neuronal simple (baseline) para clasificación multiclase en PyTorch, sin normalización ni regularización.\n"]},{"cell_type":"markdown","metadata":{"id":"160f98bf"},"source":["**Reasoning**:\n","Define the baseline neural network model as instructed, including flattening and linear layers with ReLU activation.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fb490db7","executionInfo":{"status":"ok","timestamp":1757114267363,"user_tz":360,"elapsed":11,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"95ba5195-a47a-4b40-a67b-02f4c3370c03"},"source":["import torch.nn as nn\n","\n","class BaselineNN(nn.Module):\n","    def __init__(self):\n","        super(BaselineNN, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer: 784 features, output 128 features\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(128, 64) # Hidden layer: 128 features, output 64 features\n","        self.relu2 = nn.ReLU()\n","        self.fc3 = nn.Linear(64, 10)   # Output layer: 64 features, output 10 features (for 10 classes)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.fc3(x)\n","        return x\n","\n","print(\"BaselineNN model defined.\")"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["BaselineNN model defined.\n"]}]},{"cell_type":"markdown","metadata":{"id":"5e51a17d"},"source":["## Configuración de weights & biases (w&b)\n","\n","### Subtask:\n","Configurar W&B para el seguimiento del experimento.\n"]},{"cell_type":"markdown","metadata":{"id":"2f113647"},"source":["**Reasoning**:\n","Import the wandb library and initialize a new W&B run for experiment tracking.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":277},"id":"12f7eb8d","executionInfo":{"status":"ok","timestamp":1757114292214,"user_tz":360,"elapsed":13696,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"9a2e6ae7-d969-471e-9c7f-a8f12627a72b"},"source":["import wandb\n","\n","# Initialize a new W&B run\n","wandb.init(project=\"mnist-classification\")\n","\n","print(\"W&B initialized for project 'mnist-classification'.\")"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n","  | |_| | '_ \\/ _` / _` |  _/ -_)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ma01642653\u001b[0m (\u001b[33mmansoor35\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for wandb.init()..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.21.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250905_231810-jdfr7gx1</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/mansoor35/mnist-classification/runs/jdfr7gx1' target=\"_blank\">decent-brook-1</a></strong> to <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/mansoor35/mnist-classification/runs/jdfr7gx1' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification/runs/jdfr7gx1</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["W&B initialized for project 'mnist-classification'.\n"]}]},{"cell_type":"markdown","metadata":{"id":"4dfdf004"},"source":["## Entrenamiento del modelo baseline\n","\n","### Subtask:\n","Entrenar el modelo baseline utilizando PyTorch y registrar las métricas (pérdida, precisión) con W&B.\n"]},{"cell_type":"markdown","metadata":{"id":"67395ea4"},"source":["**Reasoning**:\n","Define the loss function and optimizer, then implement the training loop to train the baseline model and log metrics to W&B.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":540},"id":"250d7319","executionInfo":{"status":"ok","timestamp":1757114364558,"user_tz":360,"elapsed":58781,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"b3a0a48e-923b-4a00-d2e4-ac253adbfa59"},"source":["import torch.optim as optim\n","\n","# Set the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Instantiate the baseline model and move it to the device\n","baseline_model = BaselineNN().to(device)\n","\n","# Define the loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = optim.Adam(baseline_model.parameters(), lr=0.001) # Using Adam optimizer with learning rate 0.001\n","\n","# Define the number of epochs\n","num_epochs = 5 # You can adjust the number of epochs\n","\n","print(\"Starting baseline model training...\")\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    baseline_model.train()  # Set the model to training mode\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    for i, data in enumerate(train_loader):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = baseline_model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        # Calculate accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_predictions += labels.size(0)\n","        correct_predictions += (predicted == labels).sum().item()\n","\n","    # Calculate average loss and accuracy for the epoch\n","    epoch_loss = running_loss / len(train_loader)\n","    epoch_accuracy = correct_predictions / total_predictions\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","    # Log training loss and accuracy to W&B\n","    wandb.log({\"epoch\": epoch + 1, \"train_loss\": epoch_loss, \"train_accuracy\": epoch_accuracy})\n","\n","print(\"Finished baseline model training.\")\n","\n","# Finalize the W&B run\n","wandb.finish()"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Starting baseline model training...\n","Epoch [1/5], Loss: 0.3507, Accuracy: 0.9026\n","Epoch [2/5], Loss: 0.1435, Accuracy: 0.9577\n","Epoch [3/5], Loss: 0.0982, Accuracy: 0.9701\n","Epoch [4/5], Loss: 0.0729, Accuracy: 0.9774\n","Epoch [5/5], Loss: 0.0581, Accuracy: 0.9814\n","Finished baseline model training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>▁▆▇██</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>0.98142</td></tr><tr><td>train_loss</td><td>0.05812</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">decent-brook-1</strong> at: <a href='https://wandb.ai/mansoor35/mnist-classification/runs/jdfr7gx1' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification/runs/jdfr7gx1</a><br> View project at: <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250905_231810-jdfr7gx1/logs</code>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"8a9480a7"},"source":["## Evaluación del modelo baseline\n","\n","### Subtask:\n","Evaluar el modelo baseline en el conjunto de prueba y registrar los resultados con W&B.\n"]},{"cell_type":"markdown","metadata":{"id":"5f2e7dfe"},"source":["**Reasoning**:\n","Initialize a new W&B run for evaluating the baseline model and then evaluate the model on the test set, calculating the accuracy and logging it to W&B before finishing the run.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439},"id":"a2ebee47","executionInfo":{"status":"ok","timestamp":1757114381000,"user_tz":360,"elapsed":3070,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"d56c8115-a700-4911-b741-feeadbff9a94"},"source":["import torch\n","import wandb\n","\n","# Initialize a new W&B run for evaluation\n","wandb.init(project=\"mnist-classification\", name=\"baseline_evaluation\")\n","\n","# Set the model to evaluation mode\n","baseline_model.eval()\n","\n","# Deactivate gradient calculation\n","with torch.no_grad():\n","    correct_predictions = 0\n","    total_predictions = 0\n","    # Iterate over the test loader\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Forward pass\n","        outputs = baseline_model(inputs)\n","\n","        # Calculate predictions\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_predictions += labels.size(0)\n","        correct_predictions += (predicted == labels).sum().item()\n","\n","    # Calculate evaluation accuracy\n","    test_accuracy = correct_predictions / total_predictions\n","\n","    # Print evaluation accuracy\n","    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","\n","    # Log evaluation accuracy to W&B\n","    wandb.log({\"test_accuracy\": test_accuracy})\n","\n","# Finalize the W&B run\n","wandb.finish()"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["creating run (0.0s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.21.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250905_231938-lcwgy8wb</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/mansoor35/mnist-classification/runs/lcwgy8wb' target=\"_blank\">baseline_evaluation</a></strong> to <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/mansoor35/mnist-classification/runs/lcwgy8wb' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification/runs/lcwgy8wb</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.9749\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.9749</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">baseline_evaluation</strong> at: <a href='https://wandb.ai/mansoor35/mnist-classification/runs/lcwgy8wb' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification/runs/lcwgy8wb</a><br> View project at: <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250905_231938-lcwgy8wb/logs</code>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"91ec99db"},"source":["## Definición del modelo con normalización y regularización\n","\n","### Subtask:\n","Definir una nueva red neuronal con normalización por lotes (Batch Normalization), regularización L2 y Dropout.\n"]},{"cell_type":"markdown","metadata":{"id":"2f9342f1"},"source":["**Reasoning**:\n","Define a new neural network class with Batch Normalization, L2 regularization (handled by the optimizer's weight_decay), and Dropout layers as specified in the instructions.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9d861939","executionInfo":{"status":"ok","timestamp":1757114394979,"user_tz":360,"elapsed":18,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"cb86b212-9211-43e1-ccdb-5df1cf4b44e0"},"source":["import torch.nn as nn\n","\n","class RegularizedNN(nn.Module):\n","    def __init__(self):\n","        super(RegularizedNN, self).__init__()\n","        self.flatten = nn.Flatten()\n","\n","        # First layer with Batch Normalization and Dropout\n","        self.fc1 = nn.Linear(28 * 28, 128)\n","        self.bn1 = nn.BatchNorm1d(128)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(0.5) # Increased dropout probability\n","\n","        # Second layer with Batch Normalization and Dropout\n","        self.fc2 = nn.Linear(128, 64)\n","        self.bn2 = nn.BatchNorm1d(64)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(0.5) # Increased dropout probability\n","\n","        # Output layer\n","        self.fc3 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.bn1(x)\n","        x = self.relu1(x)\n","        x = self.dropout1(x)\n","\n","        x = self.fc2(x)\n","        x = self.bn2(x)\n","        x = self.relu2(x)\n","        x = self.dropout2(x)\n","\n","        x = self.fc3(x)\n","        return x\n","\n","print(\"RegularizedNN model defined with Batch Normalization and Dropout.\")"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["RegularizedNN model defined with Batch Normalization and Dropout.\n"]}]},{"cell_type":"markdown","metadata":{"id":"0ec4eb27"},"source":["## Entrenamiento del modelo con normalización y regularización\n","\n","### Subtask:\n","Entrenar el modelo con normalización y regularización, registrando las métricas con W&B.\n"]},{"cell_type":"markdown","metadata":{"id":"cbf6aaf2"},"source":["**Reasoning**:\n","Import the necessary libraries for training and W&B logging, initialize a new W&B run, set the device, instantiate the regularized model, define the loss function and optimizer with L2 regularization, and then start the training loop as per the instructions.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":644},"id":"41e38923","executionInfo":{"status":"ok","timestamp":1757114477227,"user_tz":360,"elapsed":65342,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"0cf3ecba-7e6c-41fe-c8d2-77fdbec28561"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import wandb\n","\n","# Initialize a new W&B run for regularized model training\n","wandb.init(project=\"mnist-classification\", name=\"regularized_model_training\")\n","\n","# Set the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Instantiate the regularized model and move it to the device\n","regularized_model = RegularizedNN().to(device)\n","\n","# Define the loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the optimizer with L2 regularization (weight_decay)\n","optimizer = optim.Adam(regularized_model.parameters(), lr=0.001, weight_decay=1e-5) # Added weight_decay for L2 regularization\n","\n","# Define the number of epochs\n","num_epochs = 5 # You can adjust the number of epochs\n","\n","print(\"Starting regularized model training...\")\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    regularized_model.train()  # Set the model to training mode\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    for i, data in enumerate(train_loader):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = regularized_model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        # Calculate accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_predictions += labels.size(0)\n","        correct_predictions += (predicted == labels).sum().item()\n","\n","    # Calculate average loss and accuracy for the epoch\n","    epoch_loss = running_loss / len(train_loader)\n","    epoch_accuracy = correct_predictions / total_predictions\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","    # Log training loss and accuracy to W&B\n","    wandb.log({\"epoch\": epoch + 1, \"regularized_train_loss\": epoch_loss, \"regularized_train_accuracy\": epoch_accuracy})\n","\n","print(\"Finished regularized model training.\")\n","\n","# Finalize the W&B run\n","wandb.finish()"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["creating run (0.0s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.21.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250905_232011-4hs2c2na</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/mansoor35/mnist-classification/runs/4hs2c2na' target=\"_blank\">regularized_model_training</a></strong> to <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/mansoor35/mnist-classification/runs/4hs2c2na' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification/runs/4hs2c2na</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Starting regularized model training...\n","Epoch [1/5], Loss: 0.5932, Accuracy: 0.8356\n","Epoch [2/5], Loss: 0.3546, Accuracy: 0.8973\n","Epoch [3/5], Loss: 0.3053, Accuracy: 0.9121\n","Epoch [4/5], Loss: 0.2736, Accuracy: 0.9210\n","Epoch [5/5], Loss: 0.2541, Accuracy: 0.9268\n","Finished regularized model training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>regularized_train_accuracy</td><td>▁▆▇██</td></tr><tr><td>regularized_train_loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>regularized_train_accuracy</td><td>0.92677</td></tr><tr><td>regularized_train_loss</td><td>0.25408</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">regularized_model_training</strong> at: <a href='https://wandb.ai/mansoor35/mnist-classification/runs/4hs2c2na' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification/runs/4hs2c2na</a><br> View project at: <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250905_232011-4hs2c2na/logs</code>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"fb678e39"},"source":["## Evaluación del modelo con normalización y regularización\n","\n","### Subtask:\n","Evaluar el modelo con normalización y regularización en el conjunto de prueba y registrar los resultados con W&B.\n"]},{"cell_type":"markdown","metadata":{"id":"45ddfdc8"},"source":["**Reasoning**:\n","Import necessary libraries and initialize a new W&B run for evaluation.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"47422864","executionInfo":{"status":"ok","timestamp":1757114494647,"user_tz":360,"elapsed":1386,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"535b3adc-f77a-4027-e104-0508024df9a8"},"source":["import torch\n","import wandb\n","\n","# Initialize a new W&B run for evaluation of the regularized model\n","wandb.init(project=\"mnist-classification\", name=\"regularized_evaluation\")\n","\n","print(\"W&B initialized for regularized model evaluation.\")"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["creating run (0.0s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.21.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250905_232133-25nx3tlq</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/mansoor35/mnist-classification/runs/25nx3tlq' target=\"_blank\">regularized_evaluation</a></strong> to <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/mansoor35/mnist-classification/runs/25nx3tlq' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification/runs/25nx3tlq</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["W&B initialized for regularized model evaluation.\n"]}]},{"cell_type":"markdown","metadata":{"id":"261b9f75"},"source":["**Reasoning**:\n","Evaluate the regularized model on the test dataset and log the test accuracy to W&B.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"ffb6f62c","executionInfo":{"status":"ok","timestamp":1757114503911,"user_tz":360,"elapsed":3218,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"527eb965-600b-4f41-df6d-5c787683bd16"},"source":["# Set the model to evaluation mode\n","regularized_model.eval()\n","\n","# Deactivate gradient calculation\n","with torch.no_grad():\n","    correct_predictions = 0\n","    total_predictions = 0\n","    # Iterate over the test loader\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Forward pass\n","        outputs = regularized_model(inputs)\n","\n","        # Calculate predictions\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_predictions += labels.size(0)\n","        correct_predictions += (predicted == labels).sum().item()\n","\n","    # Calculate evaluation accuracy\n","    test_accuracy = correct_predictions / total_predictions\n","\n","    # Print evaluation accuracy\n","    print(f\"Regularized Model Test Accuracy: {test_accuracy:.4f}\")\n","\n","    # Log evaluation accuracy to W&B\n","    wandb.log({\"regularized_test_accuracy\": test_accuracy})\n","\n","# Finalize the W&B run\n","wandb.finish()"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Regularized Model Test Accuracy: 0.9658\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>regularized_test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>regularized_test_accuracy</td><td>0.9658</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">regularized_evaluation</strong> at: <a href='https://wandb.ai/mansoor35/mnist-classification/runs/25nx3tlq' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification/runs/25nx3tlq</a><br> View project at: <a href='https://wandb.ai/mansoor35/mnist-classification' target=\"_blank\">https://wandb.ai/mansoor35/mnist-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250905_232133-25nx3tlq/logs</code>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"3cc9e478"},"source":["## Comparación de resultados\n","\n","### Subtask:\n","Comparar los resultados de ambos modelos (baseline y con regularización/normalización) utilizando la interfaz de W&B.\n"]},{"cell_type":"markdown","metadata":{"id":"b06a3028"},"source":["## Análisis de resultados\n","\n","### Subtask:\n","Analizar el impacto de la normalización y regularización en el rendimiento del modelo.\n"]},{"cell_type":"markdown","metadata":{"id":"295c3df7"},"source":["**Reasoning**:\n","Based on the previous steps, the baseline model achieved a test accuracy of 0.9749, and the regularized model achieved a test accuracy of 0.9658. I will now compare these values and discuss the impact of normalization and regularization as per the instructions.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5b461fa5","executionInfo":{"status":"ok","timestamp":1757114524123,"user_tz":360,"elapsed":32,"user":{"displayName":"Cristobal CamHer","userId":"07487221864848747057"}},"outputId":"d7e70619-29f9-4f7a-8bae-6ebfabf27f80"},"source":["baseline_accuracy = 0.9749\n","regularized_accuracy = 0.9658\n","\n","print(f\"Baseline Model Test Accuracy: {baseline_accuracy:.4f}\")\n","print(f\"Regularized Model Test Accuracy: {regularized_accuracy:.4f}\")\n","\n","print(\"\\nComparison and Impact of Normalization and Regularization:\")\n","if regularized_accuracy > baseline_accuracy:\n","    print(\"The regularized model performed better on the test set compared to the baseline model.\")\n","elif regularized_accuracy < baseline_accuracy:\n","    print(\"The regularized model performed slightly worse on the test set compared to the baseline model.\")\n","else:\n","    print(\"The regularized model performed similarly to the baseline model on the test set.\")\n","\n","print(\"\\nDiscussion of Potential Reasons:\")\n","print(\"Normalization (Batch Normalization) is typically used to stabilize and accelerate the training process by normalizing the inputs to layers.\")\n","print(\"Regularization (L2 and Dropout) are techniques intended to prevent overfitting by adding penalties to the model complexity (L2) or randomly dropping units during training (Dropout).\")\n","print(\"In this specific case, the baseline model achieved slightly higher test accuracy. This could be due to several factors:\")\n","print(\"- The baseline model, without regularization, might have overfit the training data, but on this specific test set, the overfitting did not significantly degrade performance, or the test set is very similar to the training data distribution.\")\n","print(\"- The chosen regularization strength (L2 weight_decay) or dropout rate might be too high for this relatively simple dataset and model, causing the model to be under-capacity or hindering its ability to learn the underlying patterns effectively.\")\n","print(\"- The number of training epochs might be too small for the regularized model to fully benefit from the regularization techniques, which often require more training to converge effectively compared to unregularized models.\")\n","print(\"- The dataset (MNIST) is relatively simple, and a basic model might already achieve near-optimal performance without heavy regularization. In such cases, regularization might not provide a significant boost or could even slightly hinder performance if not carefully tuned.\")\n","print(\"For more complex datasets or models, normalization and regularization are generally crucial for improving generalization and preventing overfitting.\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline Model Test Accuracy: 0.9749\n","Regularized Model Test Accuracy: 0.9658\n","\n","Comparison and Impact of Normalization and Regularization:\n","The regularized model performed slightly worse on the test set compared to the baseline model.\n","\n","Discussion of Potential Reasons:\n","Normalization (Batch Normalization) is typically used to stabilize and accelerate the training process by normalizing the inputs to layers.\n","Regularization (L2 and Dropout) are techniques intended to prevent overfitting by adding penalties to the model complexity (L2) or randomly dropping units during training (Dropout).\n","In this specific case, the baseline model achieved slightly higher test accuracy. This could be due to several factors:\n","- The baseline model, without regularization, might have overfit the training data, but on this specific test set, the overfitting did not significantly degrade performance, or the test set is very similar to the training data distribution.\n","- The chosen regularization strength (L2 weight_decay) or dropout rate might be too high for this relatively simple dataset and model, causing the model to be under-capacity or hindering its ability to learn the underlying patterns effectively.\n","- The number of training epochs might be too small for the regularized model to fully benefit from the regularization techniques, which often require more training to converge effectively compared to unregularized models.\n","- The dataset (MNIST) is relatively simple, and a basic model might already achieve near-optimal performance without heavy regularization. In such cases, regularization might not provide a significant boost or could even slightly hinder performance if not carefully tuned.\n","For more complex datasets or models, normalization and regularization are generally crucial for improving generalization and preventing overfitting.\n"]}]},{"cell_type":"markdown","metadata":{"id":"8d902415"},"source":["## Summary:\n","\n","### Data Analysis Key Findings\n","\n","*   The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9), with 60,000 training and 10,000 test examples.\n","*   A baseline neural network without normalization or regularization achieved a test accuracy of 0.9749 on the MNIST dataset.\n","*   A regularized neural network (with Batch Normalization, L2, and Dropout) achieved a test accuracy of 0.9658 on the MNIST dataset.\n","*   In this specific experiment on the MNIST dataset, the regularized model performed slightly worse on the test set compared to the baseline model.\n","\n","### Insights or Next Steps\n","\n","*   For a relatively simple dataset like MNIST, a basic model without extensive regularization might be sufficient, and careful hyperparameter tuning is crucial when adding regularization to avoid negatively impacting performance.\n","*   Investigate the effect of different regularization strengths (L2 weight decay and dropout rates) and potentially a higher number of training epochs for the regularized model to see if performance can be improved.\n"]}]}