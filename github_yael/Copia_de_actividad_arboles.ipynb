{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0a0365e",
      "metadata": {
        "id": "e0a0365e"
      },
      "source": [
        "## Actividad √°rboles y bosques\n",
        "\n",
        "En esta pr√°ctica aprender√°s a:\n",
        "- Entrenar y afinar un √Årbol de Decisi√≥n.\n",
        "- Construir un bosque manual de √°rboles y combinar sus predicciones mediante voto mayoritario.\n",
        "- Observar c√≥mo un ensamble de modelos puede superar a un solo √°rbol."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f5e7bb",
      "metadata": {
        "id": "97f5e7bb"
      },
      "source": [
        "#### Parte 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9171ba00",
      "metadata": {
        "id": "9171ba00"
      },
      "source": [
        "1. Generar datos\n",
        "    - Crea un dataset en forma de lunas con:\n",
        "    - make_moons(n_samples=10000, noise=0.4)\n",
        "\n",
        "2. Dividir los datos\n",
        "    - Separa en conjunto de entrenamiento y de prueba usando train_test_split().\n",
        "\n",
        "3. Ajustar el modelo\n",
        "    - Usa b√∫squeda en malla (grid search) con validaci√≥n cruzada (clase GridSearchCV) para encontrar buenos hiperpar√°metros para un DecisionTreeClassifier.\n",
        "    - Pista: prueba distintos valores de max_leaf_nodes.\n",
        "\n",
        "4. Entrenar y evaluar\n",
        "    - Entrena el √°rbol con todo el conjunto de entrenamiento usando los hiperpar√°metros √≥ptimos.\n",
        "    - Eval√∫a en el conjunto de prueba.\n",
        "\n",
        "Deber√≠as obtener aproximadamente 85%‚Äì87% de precisi√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7ff68ccd",
      "metadata": {
        "id": "7ff68ccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7eb176-c04d-4ffd-c62a-241c477e4fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores hiperpar√°metros: {'max_leaf_nodes': 20}\n",
            "Precisi√≥n en el conjunto de prueba: 0.8700\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {'max_leaf_nodes': [10, 20, 30, 40, 50]}\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(tree_clf, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Mejores hiperpar√°metros: {best_params}\")\n",
        "\n",
        "final_tree_clf = DecisionTreeClassifier(**best_params, random_state=42)\n",
        "final_tree_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = final_tree_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precisi√≥n en el conjunto de prueba: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a48d04",
      "metadata": {
        "id": "56a48d04"
      },
      "source": [
        "#### Parte 2 Crecer un bosque\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3729eb6",
      "metadata": {
        "id": "b3729eb6"
      },
      "source": [
        "\n",
        "1. Generar subconjuntos\n",
        "    - Crea 1,000 subconjuntos del conjunto de entrenamiento, cada uno con 100 instancias seleccionadas aleatoriamente.\n",
        "    - Pista: usa ShuffleSplit de Scikit-Learn.\n",
        "\n",
        "2. Entrenar m√∫ltiples √°rboles\n",
        "    - Entrena un DecisionTreeClassifier en cada subconjunto, usando los mejores hiperpar√°metros encontrados en la Parte 1.\n",
        "    - Eval√∫a cada √°rbol individual en el conjunto de prueba.\n",
        "    - Como fueron entrenados en conjuntos peque√±os, se espera que tengan solo ‚âà80% de precisi√≥n.\n",
        "\n",
        "3. Combinar predicciones\n",
        "    - Para cada instancia del conjunto de prueba, recolecta las predicciones de los 1,000 √°rboles.\n",
        "    - Conservar √∫nicamente la predicci√≥n m√°s frecuente usando mode() de SciPy.\n",
        "    - Esto implementa un voto mayoritario.\n",
        "\n",
        "4. Evaluar el bosque\n",
        "    - Eval√∫a las predicciones combinadas en el conjunto de prueba.\n",
        "    - Deber√≠as obtener una precisi√≥n ligeramente mayor que la del √°rbol individual (+0.5% a +1.5%).\n",
        "\n",
        "üéâ ¬°Felicidades, has implementado tu propio Random Forest desde cero!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c15c3d34",
      "metadata": {
        "id": "c15c3d34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1027d5-6821-42a2-e0dd-86e006ce3bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisi√≥n promedio de los √°rboles individuales: 0.8012\n",
            "Precisi√≥n del bosque (voto mayoritario): 0.8720\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "from scipy.stats import mode\n",
        "import numpy as np\n",
        "\n",
        "n_trees = 1000\n",
        "n_instances = 100\n",
        "\n",
        "shuffle_split = ShuffleSplit(n_splits=n_trees, train_size=n_instances, random_state=42)\n",
        "\n",
        "# m√∫ltiples √°rboles\n",
        "individual_accuracies = []\n",
        "tree_predictions = []\n",
        "\n",
        "for train_index, test_index in shuffle_split.split(X_train):\n",
        "    X_subset = X_train[train_index]\n",
        "    y_subset = y_train[train_index]\n",
        "\n",
        "    tree_clf = DecisionTreeClassifier(**best_params, random_state=42)\n",
        "    tree_clf.fit(X_subset, y_subset)\n",
        "\n",
        "    y_pred_individual = tree_clf.predict(X_test)\n",
        "    individual_accuracies.append(accuracy_score(y_test, y_pred_individual))\n",
        "    tree_predictions.append(y_pred_individual)\n",
        "\n",
        "print(f\"Precisi√≥n promedio de los √°rboles individuales: {np.mean(individual_accuracies):.4f}\")\n",
        "\n",
        "tree_predictions = np.array(tree_predictions)\n",
        "tree_predictions_transposed = tree_predictions.T\n",
        "\n",
        "y_pred_forest, _ = mode(tree_predictions_transposed, axis=1, keepdims=False)\n",
        "\n",
        "forest_accuracy = accuracy_score(y_test, y_pred_forest)\n",
        "print(f\"Precisi√≥n del bosque (voto mayoritario): {forest_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}